# An unique identifier for the head node and workers of this cluster.
cluster_name: clark-datasets-debug-cluster

# The maximum number of workers nodes to launch in addition to the head
# node. This takes precedence over min_workers. min_workers default to 0.
# min_workers: 1
# initial_workers: 1
# max_workers: 1

# target_utilization_fraction: 0.9

# If a node is idle for this many minutes, it will be removed.
idle_timeout_minutes: 20
docker:
    image: "" # You can change this to latest-cpu if you don't need GPU support and want a faster startup
    container_name: ""
    pull_before_run: True
    run_options: []  # Extra options to pass into "docker run"

# Cloud-provider specific configuration.
provider:
    type: aws
    region: us-west-2
    # region: us-east-2
    # availability_zone: us-east-2a
    cache_stopped_nodes: False # If not present, the default is True.

# How Ray will authenticate with newly launched nodes.
auth:
    ssh_user: ubuntu
    ssh_private_key: ~/.ssh/clark-dev-autoscaler-us-west.pem

head_node_type: head

available_node_types:
    head:
        node_config:
            InstanceType: r5.4xlarge
            KeyName: clark-dev-autoscaler-us-west
            ImageId: latest_dlami
            # Set primary volume to 25 GiB
            BlockDeviceMappings:
                - DeviceName: /dev/sda1
                  Ebs:
                      VolumeSize: 300
        resources:
          test_resource: 1000

    gpu_nodes:
        min_workers: 0
        # The maximum number of worker nodes of this type to launch.
        # This takes precedence over min_workers.
        max_workers: 0
        node_config:
            InstanceType: r5.4xlarge
            KeyName: clark-dev-autoscaler-us-west
#            ImageId: latest_dlami
            IamInstanceProfile:
                Arn: arn:aws:iam::959243851260:instance-profile/ray-autoscaler-v1
            BlockDeviceMappings:
                - DeviceName: /dev/sda1
                  Ebs:
                      VolumeSize: 300
        resources:
          GPU: 1
          CPU: 0

    memory_nodes:
        min_workers: 0
        # The maximum number of worker nodes of this type to launch.
        # This takes precedence over min_workers.
        max_workers: 0
        node_config:
            InstanceType: r5.4xlarge
            KeyName: clark-dev-autoscaler-us-west
#            ImageId: latest_dlami
            IamInstanceProfile:
                Arn: arn:aws:iam::959243851260:instance-profile/ray-autoscaler-v1
            BlockDeviceMappings:
                - DeviceName: /dev/sda1
                  Ebs:
                      VolumeSize: 300
        resources:
          test_resource: 1000
    # Run workers on spot by default. Comment this out to use on-demand.
    # InstanceMarketOptions:
    #     MarketType: spot
        # SpotOptions:
        #     MaxPrice: "9.0"

file_mounts: {
    "/home/ubuntu/benchmarks": "/home/ubuntu/workspace/ray2/python/ray/data/benchmarks",
    "/home/ubuntu/.local/lib/python3.7/site-packages/ray/python/ray/data": "/home/ubuntu/workspace/ray2/python/ray/data",
}

setup_commands:
    - pip install -q boto3 tqdm torch torchvision  pyarrow fsspec s3fs==2021.08.0
    - pip uninstall ray -y && pip install -U https://s3-us-west-2.amazonaws.com/ray-wheels/master/64c2f86a225660323e499bdf6e04a6f7a3dfe7e6/ray-2.0.0.dev0-cp37-cp37m-manylinux2014_x86_64.whl

# Custom commands that will be run on the head node after common setup.
head_setup_commands: []

# Custom commands that will be run on worker nodes after common setup.
worker_setup_commands: []

# # Command to start ray on the head node. You don't need to change this.
head_start_ray_commands:
    - ray stop --force
    - rm -rf /tmp/ray/
#-  RAY_BACKEND_LOG_LEVEL=debug  ray start --head --port=6379 --object-manager-port=8076 --autoscaling-config=~/ray_bootstrap_config.yaml
    - ray start --head --port=6379 --object-manager-port=8076 --autoscaling-config=~/ray_bootstrap_config.yaml

# Command to start ray on worker nodes. You don't need to change this.
worker_start_ray_commands:
    - ray stop --force
    - rm -rf /tmp/ray/
